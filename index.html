

<!doctype html>
<html>
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="chrome=1">
      <title>MCEC algorithm | Multivariate Correlations for Early Classification</title>
      
      <link rel="stylesheet" href="stylesheets/styles.css">
      <link rel="stylesheet" href="stylesheets/pygment_trac.css">
      <meta name="viewport" content="width=device-width">
      <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->
   </head>
   <body>
      <div class="wrapper">
      <header>
         <h1>MCEC algorithm</h1>
         <p>Java implementation of an information-theoretic algorithm that combines
            Multivariate Correlations with Early Classification 
         </p>
         <p class="view"><a href="https://github.com/joaopbeirao/MCECalgorithm">View the Project on GitHub <small>MCEC algorithm</small></a></p>
         <ul>
            <li><a href="https://github.com/joaopbeirao/MCECalgorithm/raw/master/JarFile_withExample/MCECalgorithm.jar">Download<strong>JAR file</strong></a></li>
            <li><a href="https://github.com/joaopbeirao/MCECalgorithm/archive/master.zip">Download<strong>ZIP file</strong></a></li>
         </ul>
      </header>
      <section>
         <h3>Program description</h3>
         <p> MCEC (Multivariate Correlations for Early Classification) algorithm is a Java implementation of an information-theoretic 
            method for examining the early classification opportunity in a dataset. This dataset contains univariate or 
            multivariate time series together with their respective class labels. The program can be downloaded 
            <a href="https://github.com/joaopbeirao/MCECalgorithm/raw/master/JarFile_withExample/MCECalgorithm.jar">here</a>.
         </p>
         <h4>Input</h4>
         <p>
            The input file should be in comma-separated values (CSV) format, containing the time series and the respective class labels.
            <ul>
               <li>Each line is expected to correspond to one instance.</li>
               <li>The columns must include the features grouped per time point, chronologically organized. 
               The last column corresponds to the class attribute.</li>
               <li>The first line is the header, containing the names of the attributes and the associated time point. Example: 
               "Att1_TP1", "Att2_TP1", "Att1_TP2", "Att2_TP2".</li>
            </ul>
            The number of attributes (dimensions) is also required as input. The time series can be univariate 
            or multivariate, however, they must be of fixed length. Features are allowed to be categorical or numeric, 
            but the dataset cannot contain missing values, since the algorithm is not provided with any imputation procedure. 
            Furthermore, the numeric attributes must be discretized.
         </p>
         <h4>Output</h4>
         <p>
            The outcomes of the difference in entropy, log-likelihood, MDL score, AIC score and classification accuracy, 
            all for n = {1, ..., L} are outputted from the Java program in text files. 
            The implementation uses some functionalities of Weka Data Mining Software and an additional Matlab 
            script is provided for generating the five graphs for representing the results.
         </p>
         <h4>Observations</h4>
         <p> 
            The file <a href="https://github.com/joaopbeirao/MCECalgorithm/raw/master/Appendix_SyntheticExampleOfMCECalgorithm.pdf">Appendix_SyntheticExampleOfMCECalgorithm.pdf</a> 
            includes a detailed explanation of the proposed method applied to a synthetically generated dataset. 
            For clarification purposes, the functioning of the algorithm is expounded through calculation descriptions 
            and graph analysis.
         </p>
         <p>
            The proposed implementation provides the Markov Lag, an alternative to the standard Early Classification approach. 
            Basically, instead of analysing the correlations from the initial time point until the last, it uses the inverse order 
            (from the last to the first one). In this case, the idea is to check of how much information from the closest past we 
            need, in order to obtain a satisfactory prediction.
         </p>
         <h4>Libraries</h4>
         <p>
            MCEC algorithm depends on two external libraries:
               <ul><li><a href="https://poi.apache.org/download.html">poi</a>;
               <li><a href="https://www.cs.waikato.ac.nz/ml/weka/downloading.html">Weka</a>.
               </ul>
         </p>
         
         <h3>Usage</h3>
         <p>Execute the jar file:
         <pre><code>$ java -jar MCECalgorithm.jar [dataset-filename].csv [N] [optionClass] [MarkovLag]</code></pre>
         where the command-line options correspond to:
         </p>
         <pre><code>
[dataset-filename]   Type: String - Name of the dataset file to be analysed.
[N]                  Type: Integer - Number of features per time point.
[optionClass]        Type: Boolean - With classification analysis (TRUE)
                     or without classification analysis (FALSE).
[MarkovLag]          Type: Boolean - With Markov lag approach (TRUE)
                     or with standard Early Classification (FALSE).
</code></pre>
         <h4>Input file format</h4>
         <p>
            The input file should be in comma-separated values (CSV) format.
         <ul>
            <li>The first line is the header, naming the attributes and specifying the time slice index, separared by two underscores: "attributeName__t"</li>
            <li>The order of the attributes must be maintained: "a__1", "b__1", "a__2", "b__2".
            <li>The first column contains an identification (string or number) of each subject (this identifier does not affect the learnt network).</li>
            <li>All other lines correspond to observations of an individual over time.</li>
            <li>Missing values can be marked as "?" but should not occur, as the algorithm discards the observation (time slice) in question.</li>
         </ul>
         A very simplistic input file example is the following:
         </p>
         <pre><code>subject_id,resp__1,age__1,height__1,stunt__1,resp__2,age__2,height__2,stunt__2
121013, 0, 67, -3, 0, 0, 70, -3, 0
121113, 0, 27,  2, 0, 0, 30,  0, 0
121114, 0, 10,  8, 0, 0, 13,  5, 0
121140, 0, 17,  5, 0, ?,  ?,  ?, ?
(...)
</code></pre>
         <p>
            Because the algorithm assumes a multinomial distribution, data should be already discretized to guarantee a manageable (i.e., small) number of states for each attribute, taking into account the number of observations. For example, if an attribute is observed 100 times throughout time and 50 different values are recorded, this will generally not provide enough information for learning accurate data dependences.
         </p>
         
<h4>Dataset preprocessing script</h3>

<p>A Python script for coverting panel data to the tDBN input format can be downloaded <a href="https://github.com/josemonteiro/tDBN/raw/master/vtoh.py">here</a>. Its description and available command-line options are the following:</p>
         
<pre><code>usage: vtoh.py [-h] [--timeInterval TIMEINTERVAL TIMEINTERVAL]
               [--ignoreColumns IGNORECOLUMNS [IGNORECOLUMNS ...]]
               [--discretizeColumns DISCRETIZECOLUMNS [DISCRETIZECOLUMNS ...]]
               [--numberBins NUMBERBINS] [--equalFrequencyBinning]
               filename idColumn timeColumn

Converts a comma-separated-values (csv) temporal data file in 'vertical
format' (panel data format, one observation per line) to an 'horizontal
format' (one subject per line) csv file. The data file must have a column with
the identification of the subject, as well as a column with the identification
of the time instant. Both of these columns are provided as arguments and must
be integer numbers. The time interval to be considered can be specified as an
argument. It is also possible to specify columns to be ignored and columns to
be discretized (as well as the number of bins and the discretization method).

positional arguments:
  filename              Input CSV file.
  idColumn              Column containing subject IDs.
  timeColumn            Column containing time entries.

optional arguments:
  -h, --help            show this help message and exit
  --timeInterval TIMEINTERVAL TIMEINTERVAL
                        Time interval limits to be considered in the output.
  --ignoreColumns IGNORECOLUMNS [IGNORECOLUMNS ...]
                        Columns not to be included in output file.
  --discretizeColumns DISCRETIZECOLUMNS [DISCRETIZECOLUMNS ...]
                        Columns to be discretized.
  --numberBins NUMBERBINS
                        Number of bins to use when discretizing, default 10.
  --equalFrequencyBinning
                        If specified, discretization is done using equal
                        frequency or quantiles discretization (default is
                        equal width discretization).
</code></pre>
         
         <h4>Example #1</h4>
         <p>
            The first example considers a synthetic network structure with 5 attributes, each taking 8 states and with one parent from the preceding slice ([t] denotes the time-slice):
         </p>
         <p><img src="images/example1-original.png">            
         </p>
         <p>
            The above network that was sampled to produce the following observations files:
         <ul>
            <li><a href="files/synth-N50.csv">synth-N50.csv</a>, with 50 observed time transitions</li>
            <li><a href="files/synth-N150.csv">synth-N150.csv</a>, with 150 observed time transitions</li>
            <li><a href="files/synth-N250.csv">synth-N250.csv</a>, with 250 observed time transitions</li>
         </ul>
         As all nodes have exactly one parent from the past, the best options are to limit the number of parents with <code>-p 1</code> and use the log-likelihood (LL) score with <code>-s ll</code> to ensure that a maximum number of dependences is retrieved.</p>
         <p>The command to learn the network with the 50 observations file is
         </p>
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N50.csv -p 1 -s ll</code></pre>
         <p>
        and produces the following output:
         </p>
         <pre><code>Evaluating network with LL score.
Number of networks with max score: 2
Finding a maximum branching.
Network score: 202.75732463763217

-----------------

X1[0] -> X0[1]
X2[0] -> X1[1]
X3[0] -> X2[1]
X2[0] -> X3[1]
X3[0] -> X4[1]

X0[1] -> X1[1]
X3[1] -> X2[1]
X0[1] -> X3[1]
X3[1] -> X4[1]
</code></pre>
         <p>
            Activating the <code>-d</code> switch to directly output in dot format, and redirecting into <a href="http://www.graphviz.org/">Graphviz</a>...
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N50.csv -p 1 -s ll -d | dot -Tpng -o N50.png</code></pre>
         <p>...produces this graph:</p>
         <p><img src="images/example1-N50.png"></p>
         <p>
            Although there are some matches (X2[0]->X3[1] and X3[0]->X4[1]), the learnt network is very different from the original, because the number of observations is low.
         </p>
         <p>
            Using the 250 observations file as input to tDBN ...
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N250.csv -p 1 -s ll</code></pre>
         ... results in the following output, which is a perfect match of the original network:
         </p>
         <pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Finding a maximum branching.
Network score: 829.3579534531533

-----------------

X0[0] -> X0[1]
X1[0] -> X1[1]
X4[0] -> X2[1]
X2[0] -> X3[1]
X3[0] -> X4[1]

X1[1] -> X0[1]
X1[1] -> X2[1]
X2[1] -> X3[1]
X2[1] -> X4[1]
</code></pre>
         <p>An improved visual representation shows the evolution of the learnt networks with the number of observations N (dashed edges are incorrect):
         </p>
         <center>
         <table>
         <tr>
         <td><center><img src="images/N50.png"></center></td>
         <td><center><img src="images/N150.png"></center></td>
         <td><center><img src="images/N250.png"></center></td>
         </tr>
         <tr>
         <td><center>N=50</center></td>
         <td><center>N=150</center></td>
         <td><center>N=250</center></td>
         </table>
         </center>
         <h4>Example #2</h4>
         <p>
         In the second example, tDBN is employed to learn a gene regulatory network. Specifically, it uses gene expression data related to the embryonic stage of Drosophila melanogaster [<a href="#ref1">1</a>]. The dataset was preprocessed as described in [<a href="#ref2">2</a>] and can be downloaded <a href="files/drosophila-embryonic.csv">here</a>. </p>
         
         <p>As intra-slice dependences in regulatory networks are usually not reported, the option <code>-c</code> omits them by outputting the network in compact form, where each edge represents a dependence between a node at time slice t+1 and its parent at the previous time slice t.</p>
         
         <p>For this example, each node will have at most two parents from the previous slice, which is enforced with <code>-p 2</code>. The minimum description length (MDL) criterion further limits the number of parents by preferring simpler network structures. The <code>-s mdl</code> option is currently redundant, as tDBN uses MDL by default.</p>
         
         <p>The learning command is thus</p>
         <pre><code>$ java -jar tDBN-0.1.3.jar -i drosophila-embryonic.csv -p 2 -d -c | \
  dot -Tpng -o drosophila.png</code></pre>
  
        <p>and the resulting network is:</p>
         <p><img src="images/example2-drosophila-e.png"></p>
<p>Six out of the eight retrieved regulations (excluding the self-loops) are also reported in [<a href="#ref3">3</a>], indicating a potentially good result of tDBN. It should be noted, however, that data preprocessing may have not been the same, and that the number of observations is quite low, thus leading to many networks with maximum score.
            
            
            <h4>Example #3</h4>
            
            <p>
            The last example shows how to output the network parameters and how to learn a non-stationary DBN. The dataset is very simple, consisting of 2 binary attributes measured in 7 individuals over 3 time-slices, and can be downloaded <a href="files/example3.csv">here</a>:
         </p>
         <pre><code>id,a__0,b__0,a__1,b__1,a__2,b__2
1,0,0,0,1,1,1
2,0,1,1,0,1,0
3,0,1,0,0,1,0
4,1,1,1,1,1,0
5,1,1,0,1,1,0
6,1,1,0,1,0,0
7,0,0,0,0,0,0
</code></pre>

<p>To learn both the structure and the parameters of a stationary network, the following command is used:</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters</code></pre>
<p>resulting in</p>
<pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Finding a maximum branching.
Network score: 1.4715189255145624

-----------------

a[0] -> a[1]
a[0] -> b[1]

b[1] -> a[1]


a: [0.0, 1.0]
[a[0]=1.0, b[1]=0.0]: 0.500 0.500
[a[0]=0.0, b[1]=1.0]: 1.000 0.000
[a[0]=1.0, b[1]=1.0]: 0.667 0.333
[a[0]=0.0, b[1]=0.0]: 0.667 0.333

b: [0.0, 1.0]
[a[0]=0.0]: 0.750 0.250
[a[0]=1.0]: 0.000 1.000
</code></pre>

<p>For every attribute (in this case, "a" and "b"), its conditional probability distribution table is present in the output. For a given table, the probability of each state of the corresponding attribute is specified, conditioned on the configurations of its parents (one per line).</p>

<p>Proceeding to learn a non-stationary network...</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters --nonStationary</code></pre>
<p>... the output is:</p>
<pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Number of networks with max score: 2
Finding a maximum branching.
Network score: 0.3397980735907944
Network score: 0.5924696128065006

-----------------

b[0] -> a[1]
a[0] -> b[1]

a[1] -> b[1]


a: [0.0, 1.0]
[b[0]=1.0]: 0.600 0.400
[b[0]=0.0]: 1.000 0.000

b: [0.0, 1.0]
[a[0]=1.0, a[1]=0.0]: 0.000 1.000
[a[0]=1.0, a[1]=1.0]: 0.000 1.000
[a[0]=0.0, a[1]=1.0]: 1.000 0.000
[a[0]=0.0, a[1]=0.0]: 0.667 0.333

-----------------

a[1] -> a[2]
b[1] -> b[2]

b[2] -> a[2]


a: [0.0, 1.0]
[a[0]=1.0, b[1]=0.0]: 0.000 1.000
[a[0]=0.0, b[1]=1.0]: 0.000 1.000
[a[0]=1.0, b[1]=1.0]: 0.500 0.500
[a[0]=0.0, b[1]=0.0]: 0.500 0.500

b: [0.0, 1.0]
[b[0]=1.0]: 0.750 0.250
[b[0]=0.0]: 1.000 0.000
</code></pre>
<p>Or, outputting to Graphviz...</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters \
--nonStationary | dot -Tpng -o example3.png</code></pre>
<p>... this graph is produced:</p>
<p><img src="images/example3.png"></p>
            
            <!--Advanced usage: tDBN-0.1.3.jar uses the class com.github.tDBN.cli.LearnFromFile as entry point. Other entry points exist.-->
            
         <h3><a name="algorithm"></a>The tDBN learning algorithm</h3>
         <p>
            The tDBN algorithm jointly learns the optimal intra and inter time-slice connectivity of a DBN by constraining the search space to tree augmented networks.  In a tree-augmented network, attributes (nodes) have at most one parent in the same time-slice, hence the intra-slice connectivity is a tree. An attribute can however have several parents from from preceding slices.
         </p>
         <p>
            The following figure illustrates the structure of a tree-augmented network, with 5 attributes and one parent from the previous time-slice:</p>
            <p><img src="images/tree-augmented.png"></p>


<h3>Assessment on time complexity</h3>
        <p>
        The tDBN learning algorithm has a theoretical time complexity of O(n<sup>p+3</sup> r<sup>p+2</sup> N), where n is the number of network attributes, p is the number of parents from the preceding time-slice, r is the number of states of all attributes and N is the number of observations.</p>
        <p>This section presents the results of a set of simulations to assess the algorithm's running time versus its theoretical complexity.  The methodology consisted in 1) generating random tree-augmented networks, 2) sampling each of those networks to generate observations, and 3) inputting the observations to the tDBN algorithm to recover the underlying structure. The time taken by the third step of this process &mdash; the algorithm's running time &mdash; was recorded. Furthermore, the original and recovered networks were compared by evaluating the precision metric.</p>
        
        <p>Each of the following subsections studies the effect of varying one of the parameters (n, p, r or N), keeping all the others constant. The default values of the parameters are:</p>
        <table>
        <tr><td>n</td><td>10</td></td></tr>
        <tr><td>p</td><td>2</td></td></tr>
        <tr><td>r</td><td>5</td></td></tr>
        <tr><td>N</td><td>100</td></td></tr>
        </table>
        <p>The results are displayed as average statistics over 10 runs, also indicating the standard deviation. All simulations were run on Intel i5-3570 @ 3.40 GHz machines.</p>


        <h4>Number of attributes n</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>n</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
5</td><td>0&plusmn;0</td><td>60&plusmn;16</td></tr>
<tr><td>
8</td><td>2&plusmn;0</td><td>38&plusmn;11</td></tr>
<tr><td>
11</td><td>10&plusmn;0</td><td>27&plusmn;7</td></tr>
<tr><td>
14</td><td>31&plusmn;0</td><td>17&plusmn;6</td></tr>
<tr><td>
17</td><td>81&plusmn;1</td><td>14&plusmn;5</td></tr>
<tr><td>
20</td><td>175&plusmn;1</td><td>14&plusmn;4</td></tr>
<tr><td>
23</td><td>326&plusmn;2</td><td>8&plusmn;4</td></tr>
<tr><td>
26</td><td>581&plusmn;2</td><td>9&plusmn;4</td></tr>
<tr><td>
29</td><td>976&plusmn;4</td><td>9&plusmn;6</td></tr>
<tr><td>
32</td><td>1555&plusmn;3</td><td>5&plusmn;2</td></tr>
<tr><td>
35</td><td>2389&plusmn;9</td><td>8&plusmn;2</td></tr>
        </table></p>
        
        <p><img src="images/n-variation.png"></p>
                <p>The adjustment of a 4-degree polynomial curve to the obtained data (Time vs. n) yields an almost perfect fit, with R&sup2; > 0.9999. This result is in line with the theoretical complexity of O(n<sup>5</sup>), assuming other parameters constant. As n tends to larger values, only a 5-degree polynomial curve is expected to perfectly fit the data.</p>
        <p><table>
        <colgroup><col width="40"/><col width="40"/></colgroup>
        <tr><td>Degree of the fitting polynomial</td><td>R&sup2;</td></tr>
        <tr><td>1</td><td>0.699</td></td></tr>
        <tr><td>2</td><td>0.9636</td></td></tr>
        <tr><td>3</td><td>0.9985</td></td></tr>
        <tr><td>4</td><td>0.99998</td></td></tr>
        <tr><td>5</td><td>0.999997</td></td></tr>
        </table>

        
        <h4>Number of parents p</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>p</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
1</td><td>0&plusmn;0</td><td>69&plusmn;12</td></tr>
<tr><td>
2</td><td>7&plusmn;0</td><td>25&plusmn;8</td></tr>
<tr><td>
3</td><td>79&plusmn;1</td><td>26&plusmn;7</td></tr>
<tr><td>
4</td><td>671&plusmn;18</td><td>36&plusmn;5</td></tr>
<tr><td>
5</td><td>4395*</td><td>44*</td></tr>
        </table>
        <small>* Due to large running time, simulation for p=5 was only run once</small></p>
        
        <p><img src="images/p-variation.png">&nbsp;<img src="images/p-variation-log.png"></p>
        
        <table><tr><td>Exponential fit</td><td>R&sup2; = 0.993</td></tr></table>
            
            <p>The adjustment of an exponential curve to the obtained data (Time vs. p) yields an good fit, with R&sup2; > 0.99. It can be observed in the semi-log graph that the real curve is not a straight line, but has a slight inclination downwards. This suggests that the real complexity is sub-exponential, nevertheless being bounded by O(2<sup>p</sup>), assuming other parameters constant.</p>
        
        <h4>Size of attributes r</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>p</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
2</td><td>0&plusmn;0</td><td>55&plusmn;12</td></tr>
<tr><td>
4</td><td>3&plusmn;0</td><td>37&plusmn;12</td></tr>
<tr><td>
6</td><td>12&plusmn;0</td><td>22&plusmn;9</td></tr>
<tr><td>
8</td><td>33&plusmn;1</td><td>20&plusmn;7</td></tr>
<tr><td>
10</td><td>74&plusmn;1</td><td>22&plusmn;9</td></tr>
<tr><td>
12</td><td>147&plusmn;2</td><td>13&plusmn;7</td></tr>
<tr><td>
14</td><td>259&plusmn;4</td><td>13&plusmn;4</td></tr>
<tr><td>
16</td><td>428&plusmn;3</td><td>17&plusmn;6</td></tr>
<tr><td>
18</td><td>676&plusmn;5</td><td>17&plusmn;5</td></tr>
<tr><td>
20</td><td>1003&plusmn;7</td><td>18&plusmn;8</td></tr>
        </table></p>
        
        <p><img src="images/r-variation.png"></p>
        <p>The adjustment of a 3-degree polynomial curve to the obtained data (Time vs. r) yields an almost perfect fit, with R&sup2; > 0.9999. This result is in line with the theoretical complexity of O(r<sup>4</sup>), assuming other parameters constant. As n tends to larger values, only a 4-degree polynomial curve is expected to perfectly fit the data.</p>
        <p><table>
        <colgroup><col width="40"/><col width="40"/></colgroup>
        <tr><td>Degree of the fitting polynomial</td><td>R&sup2;</td></tr>
        <tr><td>1</td><td>0.794</td></td></tr>
        <tr><td>2</td><td>0.9882</td></td></tr>
        <tr><td>3</td><td>0.99991</td></td></tr>
        <tr><td>4</td><td>0.99998</td></td></tr>
        </table>
        
        <h4>Number of observations N</h4>

        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>N</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
100</td><td>6&plusmn;0</td><td>32&plusmn;6</td></tr>
<tr><td>
200</td><td>13&plusmn;0</td><td>54&plusmn;16</td></tr>
<tr><td>
300</td><td>20&plusmn;1</td><td>85&plusmn;15</td></tr>
<tr><td>
400</td><td>27&plusmn;1</td><td>97&plusmn;4</td></tr>
<tr><td>
500</td><td>38&plusmn;1</td><td>99&plusmn;1</td></tr>
<tr><td>
600</td><td>47&plusmn;1</td><td>100&plusmn;1</td></tr>
<tr><td>
700</td><td>52&plusmn;2</td><td>99&plusmn;3</td></tr>
<tr><td>
800</td><td>59&plusmn;2</td><td>100&plusmn;1</td></tr>
<tr><td>
900</td><td>70&plusmn;2</td><td>100&plusmn;0</td></tr>
<tr><td>
1000</td><td>77&plusmn;2</td><td>99&plusmn;2</td></tr>
        </table></p>
        
                <p><img src="images/N-variation.png"></p>
                
                <p>The adjustment of a linear curve to the obtained data (Time vs. N) yields a good fit, with R&sup2; > 0.99. This result is in line with the theoretical complexity of O(N), assuming other parameters constant.</p>
                
            <table><tr><td>Linear fit</td><td>R&sup2; = 0.9971</td></tr></table>


         <h3>References</h3>
         <ol>
            <li><a name="ref1"></a>Arbeitman, Michelle N., et al. "Gene expression during the life cycle of Drosophila melanogaster." Science 297.5590 (2002): 2270-2275.</li>
            <li><a name="ref2"></a>Zhao, Wentao, Erchin Serpedin, and Edward R. Dougherty. "Inferring gene regulatory networks from time series data using the minimum description length principle." Bioinformatics 22.17 (2006): 2129-2135.</li>
            <li><a name="ref3"></a>Dondelinger, Frank, Sophie Lèbre, and Dirk Husmeier. "Non-homogeneous dynamic Bayesian networks with Bayesian regularization for inferring gene regulatory networks with gradually time-varying structure." Machine learning 90.2 (2013): 191-230.</li>
         </ol>
         
      </section>
      <footer>
         <p>This project is maintained by <a href="https://github.com/josemonteiro">josemonteiro</a></p>
         <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
      </div>
   </body>
</html>

